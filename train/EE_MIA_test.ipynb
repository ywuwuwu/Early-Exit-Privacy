{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abe50a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 16:08:39.251610: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-12 16:08:40.132761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 16:08:40.132830: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-12 16:08:40.132839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.base import BranchModel\n",
    "from models.costs import module_cost\n",
    "\n",
    "import logging\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define the path to the saved model\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "import onnx\n",
    "import onnx_tf\n",
    "import tensorflow as tf\n",
    "\n",
    "# from base.evaluators import standard_eval, branches_eval, binary_eval, \\\n",
    "#     binary_statistics\n",
    "# from models.base import BranchModel\n",
    "# from utils import get_device\n",
    "# from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9452c1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7421d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class EarlyExitBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, exit_threshold):\n",
    "        super(EarlyExitBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.exit_threshold = exit_threshold\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        pred = self.softmax(x)\n",
    "        if torch.max(pred, 1)[0] > self.exit_threshold:\n",
    "            return pred\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "class EarlyExitNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, exit_thresholds):\n",
    "        super(EarlyExitNetwork, self).__init__()\n",
    "        self.exit_thresholds = exit_thresholds\n",
    "        self.block1 = EarlyExitBlock(input_size, hidden_size, output_size, exit_thresholds[0])\n",
    "        self.block2 = EarlyExitBlock(hidden_size, hidden_size, output_size, exit_thresholds[1])\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        if x is not None:\n",
    "            return x\n",
    "        x = self.block2(x)\n",
    "        if x is not None:\n",
    "            return x\n",
    "        x = self.fc(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize the network and optimizer\n",
    "input_size = 784\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "exit_thresholds = [0.9, 0.8]\n",
    "net = EarlyExitNetwork(input_size, hidden_size, output_size, exit_thresholds)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Load the dataset and create data loaders\n",
    "train_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Train the network\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, input_size)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = nn.functional.nll_loss(torch.log(outputs), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch %d: loss=%.3f' % (epoch+1, running_loss/len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16fa0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model: BranchModel, \n",
    "            predictors: nn.Module,\n",
    "            optimizer,\n",
    "            train_loader,\n",
    "            epochs,\n",
    "            device,\n",
    "            scheduler=None,\n",
    "            early_stopping=None,\n",
    "            test_loader=None, eval_loader=None):\n",
    "\n",
    "    scores = []\n",
    "    mean_losses = []\n",
    "\n",
    "    best_model = model.state_dict()\n",
    "    best_model_i = 0\n",
    "    best_eval_score = -1\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    if early_stopping is not None:\n",
    "        early_stopping.reset()\n",
    "\n",
    "    model.train()\n",
    "    bar = tqdm(range(epochs), leave=True)\n",
    "    for epoch in bar:\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)[-1]\n",
    "            pred = predictors[-1].logits(pred)\n",
    "\n",
    "            loss = nn.functional.cross_entropy(pred, y, reduction='none')\n",
    "            losses.extend(loss.tolist())\n",
    "            loss = loss.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        mean_losses.append(mean_loss)\n",
    "        \n",
    "        if early_stopping is not None:\n",
    "            r = early_stopping.step(eval_scores) if eval_loader is not None \\\n",
    "                else early_stopping.step(mean_loss)\n",
    "\n",
    "            if r < 0:\n",
    "                break\n",
    "            elif r > 0:\n",
    "                best_model = deepcopy(model.state_dict())\n",
    "                best_predictors = deepcopy(predictors.state_dict())\n",
    "\n",
    "                best_model_i = epoch\n",
    "        else:\n",
    "            if (eval_scores is not None and eval_scores >= best_eval_score) \\\n",
    "                    or eval_scores is None:\n",
    "\n",
    "                if eval_scores is not None:\n",
    "                    best_eval_score = eval_scores\n",
    "\n",
    "                best_model = deepcopy(model.state_dict())\n",
    "                best_predictors = deepcopy(predictors.state_dict())\n",
    "\n",
    "                best_model_i = epoch\n",
    "                train_scores = standard_eval(model=model,\n",
    "                                     dataset_loader=train_loader,\n",
    "                                     classifier=predictors[-1])\n",
    "\n",
    "        test_scores = standard_eval(model=model,\n",
    "                                    dataset_loader=test_loader,\n",
    "                                    classifier=predictors[-1])\n",
    "\n",
    "        bar.set_postfix(\n",
    "            {'Train score': train_scores, 'Test score': test_scores,\n",
    "             'Eval score': eval_scores if eval_scores != 0 else 0,\n",
    "             'Mean loss': mean_loss})\n",
    "\n",
    "        scores.append((train_scores, eval_scores, test_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227a6e20",
   "metadata": {},
   "source": [
    "## FL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4302adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid\n",
    "from utils.options import args_parser\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "from models.Fed import FedAvg\n",
    "from models.test import test_img\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args = args_parser()\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "    # load dataset and split users\n",
    "    if args.dataset == 'mnist':\n",
    "        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "        # sample users\n",
    "        if args.iid:\n",
    "            dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "    elif args.dataset == 'cifar':\n",
    "        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "        dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "        dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "        if args.iid:\n",
    "            dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            exit('Error: only consider IID setting in CIFAR10')\n",
    "    else:\n",
    "        exit('Error: unrecognized dataset')\n",
    "    img_size = dataset_train[0][0].shape\n",
    "\n",
    "    # build model\n",
    "    if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "        net_glob = CNNCifar(args=args).to(args.device)\n",
    "    elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "        net_glob = CNNMnist(args=args).to(args.device)\n",
    "    elif args.model == 'mlp':\n",
    "        len_in = 1\n",
    "        for x in img_size:\n",
    "            len_in *= x\n",
    "        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "    else:\n",
    "        exit('Error: unrecognized model')\n",
    "    print(net_glob)\n",
    "    net_glob.train()\n",
    "\n",
    "    # copy weights\n",
    "    w_glob = net_glob.state_dict()\n",
    "\n",
    "    # training\n",
    "    loss_train = []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    net_best = None\n",
    "    best_loss = None\n",
    "    val_acc_list, net_list = [], []\n",
    "\n",
    "    if args.all_clients: \n",
    "        print(\"Aggregation over all clients\")\n",
    "        w_locals = [w_glob for i in range(args.num_users)]\n",
    "    for iter in range(args.epochs):\n",
    "        loss_locals = []\n",
    "        if not args.all_clients:\n",
    "            w_locals = []\n",
    "        m = max(int(args.frac * args.num_users), 1)\n",
    "        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "        for idx in idxs_users:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "            if args.all_clients:\n",
    "                w_locals[idx] = copy.deepcopy(w)\n",
    "            else:\n",
    "                w_locals.append(copy.deepcopy(w))\n",
    "            loss_locals.append(copy.deepcopy(loss))\n",
    "        # update global weights\n",
    "        w_glob = FedAvg(w_locals)\n",
    "\n",
    "        # copy weight to net_glob\n",
    "        net_glob.load_state_dict(w_glob)\n",
    "\n",
    "        # print loss\n",
    "        loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "        print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "        loss_train.append(loss_avg)\n",
    "\n",
    "    # plot loss curve\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(loss_train)), loss_train)\n",
    "    plt.ylabel('train_loss')\n",
    "    plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "\n",
    "    # testing\n",
    "    net_glob.eval()\n",
    "    acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "    print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "    print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da997da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c5b3d",
   "metadata": {},
   "source": [
    "## MIA Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9beae",
   "metadata": {},
   "source": [
    "https://github.com/spring-epfl/mia \n",
    "https://github.com/tensorflow/privacy/tree/master/tensorflow_privacy/privacy/privacy_tests/membership_inference_attack\n",
    "\n",
    "with tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ecbaa23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load global cnn model from FL\n",
    "\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "\n",
    "import argparse\n",
    "\n",
    "# Create an ArgumentParser object\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='number of classes')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# set the value of num_classes manually\n",
    "args.num_classes = 10\n",
    "\n",
    "model = CNNCifar(args)\n",
    "model.load_state_dict(torch.load('results/models/cnn'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcc32618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             456\n",
      "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 10, 10]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 5, 5]               0\n",
      "            Linear-5                  [-1, 120]          48,120\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.06\n",
      "Params size (MB): 0.24\n",
      "Estimated Total Size (MB): 0.31\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "459472d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 3, 32, 32, requires_grad=True)\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model,               # model being run\n",
    "                  x,                         # model input (or a tuple for multiple inputs)\n",
    "                  \"cnn.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9c0769d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorflowRep' object has no attribute 'members'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmembers\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorflowRep' object has no attribute 'members'"
     ]
    }
   ],
   "source": [
    "prepare(onnx_model).members()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7abe3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnx_tf.backend import prepare\n",
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"cnn.onnx\")\n",
    "\n",
    "# Convert the model to TensorFlow format\n",
    "tf_model = prepare(onnx_model) # Import the ONNX model to Tensorflow\n",
    "\n",
    "# # Save the model in protobuf format using write_graph()\n",
    "# tf.io.write_graph(tf_model.graph.as_graph_def(), 'results/models', 'tf_cnn.pb', as_text=False)\n",
    "\n",
    "# # Save the model in saved_model format using saved_model.save()\n",
    "# tf.saved_model.save(tf_model, 'results/models/tf_cnn')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "563a5a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_federated\n",
      "  Downloading tensorflow_federated-0.48.0-py2.py3-none-any.whl (42.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 42.8 MB 19.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jaxlib==0.3.14\n",
      "  Downloading jaxlib-0.3.14-cp39-none-manylinux2014_x86_64.whl (71.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 71.3 MB 101.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio~=1.46\n",
      "  Downloading grpcio-1.51.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 104.8 MB/s eta 0:00:01     |███████████████▏                | 2.3 MB 104.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jax==0.3.14\n",
      "  Downloading jax-0.3.14.tar.gz (990 kB)\n",
      "\u001b[K     |████████████████████████████████| 990 kB 90.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow~=2.11.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow_federated) (2.11.0)\n",
      "Collecting semantic-version~=2.6\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pytype==2022.12.15\n",
      "  Downloading pytype-2022.12.15-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 96.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm~=4.64 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow_federated) (4.64.0)\n",
      "Collecting tensorflow-compression~=2.11.0\n",
      "  Downloading tensorflow_compression-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 117.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-model-optimization==0.7.3\n",
      "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
      "\u001b[K     |████████████████████████████████| 238 kB 116.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.21 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow_federated) (1.21.5)\n",
      "Requirement already satisfied: absl-py==1.*,>=1.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow_federated) (1.0.0)\n",
      "Collecting typing-extensions~=4.4.0\n",
      "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting cachetools~=3.1\n",
      "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: dm-tree==0.1.7 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow_federated) (0.1.7)\n",
      "Collecting portpicker~=1.5\n",
      "  Downloading portpicker-1.5.2-py3-none-any.whl (14 kB)\n",
      "Collecting tensorflow-privacy==0.8.6\n",
      "  Downloading tensorflow_privacy-0.8.6-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 97.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs~=21.4 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow_federated) (21.4.0)\n",
      "Requirement already satisfied: dp-accounting==0.3.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow_federated) (0.3.0)\n",
      "Collecting farmhashpy==0.4.0\n",
      "  Downloading farmhashpy-0.4.0-cp39-cp39-manylinux2010_x86_64.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 115.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/yashuo/anaconda3/lib/python3.9/site-packages (from absl-py==1.*,>=1.0->tensorflow_federated) (1.16.0)\n",
      "Requirement already satisfied: scipy~=1.7.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from dp-accounting==0.3.0->tensorflow_federated) (1.7.3)\n",
      "Requirement already satisfied: mpmath~=1.2.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from dp-accounting==0.3.0->tensorflow_federated) (1.2.1)\n",
      "Requirement already satisfied: opt_einsum in /home/yashuo/anaconda3/lib/python3.9/site-packages (from jax==0.3.14->tensorflow_federated) (3.3.0)\n",
      "Requirement already satisfied: etils[epath] in /home/yashuo/anaconda3/lib/python3.9/site-packages (from jax==0.3.14->tensorflow_federated) (1.0.0)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libcst>=0.4.5\n",
      "  Downloading libcst-0.4.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 110.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jinja2>=3.1.2\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 108.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlab>=0.8\n",
      "  Downloading importlab-0.8-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: networkx<2.8.4 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from pytype==2022.12.15->tensorflow_federated) (2.7.1)\n",
      "Collecting pydot>=1.4.2\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting tabulate>=0.8.10\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting ninja>=1.10.0.post2\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "\u001b[K     |████████████████████████████████| 145 kB 99.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from pytype==2022.12.15->tensorflow_federated) (0.10.2)\n",
      "Requirement already satisfied: matplotlib~=3.3 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-privacy==0.8.6->tensorflow_federated) (3.5.1)\n",
      "Requirement already satisfied: pandas~=1.4 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-privacy==0.8.6->tensorflow_federated) (1.4.2)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.4 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-privacy==0.8.6->tensorflow_federated) (2.11.0)\n",
      "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-privacy==0.8.6->tensorflow_federated) (1.0.2)\n",
      "Requirement already satisfied: tensorflow-datasets~=4.5 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-privacy==0.8.6->tensorflow_federated) (4.8.3)\n",
      "Requirement already satisfied: tensorflow-probability==0.15.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-privacy==0.8.6->tensorflow_federated) (0.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.8.6->tensorflow_federated) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.8.6->tensorflow_federated) (2.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-probability==0.15.0->tensorflow-privacy==0.8.6->tensorflow_federated) (2.0.0)\n",
      "Requirement already satisfied: decorator in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-probability==0.15.0->tensorflow-privacy==0.8.6->tensorflow_federated) (5.1.1)\n",
      "Requirement already satisfied: gast>=0.3.2 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-probability==0.15.0->tensorflow-privacy==0.8.6->tensorflow_federated) (0.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from jinja2>=3.1.2->pytype==2022.12.15->tensorflow_federated) (2.0.1)\n",
      "Collecting typing-inspect>=0.4.0\n",
      "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: pyyaml>=5.2 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from libcst>=0.4.5->pytype==2022.12.15->tensorflow_federated) (6.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.3->tensorflow-privacy==0.8.6->tensorflow_federated) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.3->tensorflow-privacy==0.8.6->tensorflow_federated) (22.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.3->tensorflow-privacy==0.8.6->tensorflow_federated) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.3->tensorflow-privacy==0.8.6->tensorflow_federated) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.3->tensorflow-privacy==0.8.6->tensorflow_federated) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.3->tensorflow-privacy==0.8.6->tensorflow_federated) (9.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from matplotlib~=3.3->tensorflow-privacy==0.8.6->tensorflow_federated) (2.8.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytz>=2020.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from pandas~=1.4->tensorflow-privacy==0.8.6->tensorflow_federated) (2021.3)\n",
      "Requirement already satisfied: psutil in /home/yashuo/anaconda3/lib/python3.9/site-packages (from portpicker~=1.5->tensorflow_federated) (5.8.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (3.6.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (1.6.3)\n",
      "Requirement already satisfied: setuptools in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (61.2.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (2.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (2.2.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (2.11.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (15.0.6.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (0.2.0)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 95.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (0.31.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow~=2.11.0->tensorflow_federated) (1.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow~=2.11.0->tensorflow_federated) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (1.33.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (2.27.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow~=2.11.0->tensorflow_federated) (3.2.2)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-datasets~=4.5->tensorflow-privacy==0.8.6->tensorflow_federated) (1.12.0)\n",
      "Requirement already satisfied: click in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-datasets~=4.5->tensorflow-privacy==0.8.6->tensorflow_federated) (8.0.4)\n",
      "Requirement already satisfied: promise in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-datasets~=4.5->tensorflow-privacy==0.8.6->tensorflow_federated) (2.3)\n",
      "Requirement already satisfied: importlib_resources in /home/yashuo/anaconda3/lib/python3.9/site-packages (from etils[epath]->jax==0.3.14->tensorflow_federated) (5.12.0)\n",
      "Requirement already satisfied: zipp in /home/yashuo/anaconda3/lib/python3.9/site-packages (from etils[epath]->jax==0.3.14->tensorflow_federated) (3.7.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from typing-inspect>=0.4.0->libcst>=0.4.5->pytype==2022.12.15->tensorflow_federated) (0.4.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/yashuo/anaconda3/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow-datasets~=4.5->tensorflow-privacy==0.8.6->tensorflow_federated) (1.53.0)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.3.14-py3-none-any.whl size=1147584 sha256=38faacf5f841538e9980579793e00e01d7b8a0b9cff4bc7532b54412c230beec\n",
      "  Stored in directory: /home/yashuo/.cache/pip/wheels/32/21/2b/29f2d0dba28673825c67ce8451e44b07ca7bbf8e68964a82db\n",
      "Successfully built jax\n",
      "Installing collected packages: cachetools, typing-extensions, protobuf, grpcio, typing-inspect, flatbuffers, tabulate, pydot, ninja, libcst, jinja2, importlab, tensorflow-privacy, tensorflow-model-optimization, tensorflow-compression, semantic-version, pytype, portpicker, jaxlib, jax, farmhashpy, tensorflow-federated\n",
      "  Attempting uninstall: cachetools\n",
      "    Found existing installation: cachetools 4.2.2\n",
      "    Uninstalling cachetools-4.2.2:\n",
      "      Successfully uninstalled cachetools-4.2.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.1.1\n",
      "    Uninstalling typing-extensions-4.1.1:\n",
      "      Successfully uninstalled typing-extensions-4.1.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 23.1.21\n",
      "    Uninstalling flatbuffers-23.1.21:\n",
      "      Successfully uninstalled flatbuffers-23.1.21\n",
      "  Attempting uninstall: tabulate\n",
      "    Found existing installation: tabulate 0.8.9\n",
      "    Uninstalling tabulate-0.8.9:\n",
      "      Successfully uninstalled tabulate-0.8.9\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: tensorflow-privacy\n",
      "    Found existing installation: tensorflow-privacy 0.8.7\n",
      "    Uninstalling tensorflow-privacy-0.8.7:\n",
      "      Successfully uninstalled tensorflow-privacy-0.8.7\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\u001b[0m\n",
      "Successfully installed cachetools-3.1.1 farmhashpy-0.4.0 flatbuffers-2.0.7 grpcio-1.51.3 importlab-0.8 jax-0.3.14 jaxlib-0.3.14 jinja2-3.1.2 libcst-0.4.9 ninja-1.11.1 portpicker-1.5.2 protobuf-3.19.6 pydot-1.4.2 pytype-2022.12.15 semantic-version-2.10.0 tabulate-0.9.0 tensorflow-compression-2.11.0 tensorflow-federated-0.48.0 tensorflow-model-optimization-0.7.3 tensorflow-privacy-0.8.6 typing-extensions-4.4.0 typing-inspect-0.8.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_federated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91508cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 5s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 12:29:44.561205: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 153600000 exceeds 10% of free system memory.\n",
      "2023-03-02 12:29:44.666274: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 153600000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_federated.python.learning' has no attribute 'build_federated_averaging_process'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [55]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     keras_model \u001b[38;5;241m=\u001b[39m create_keras_model()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tff\u001b[38;5;241m.\u001b[39mlearning\u001b[38;5;241m.\u001b[39mfrom_keras_model(keras_model, input_spec\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39melement_spec, loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(), metrics\u001b[38;5;241m=\u001b[39m[tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()])\n\u001b[0;32m---> 27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_federated_averaging_process\u001b[49m(model_fn)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model using the TFF simulation\u001b[39;00m\n\u001b[1;32m     30\u001b[0m state \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39minitialize()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow_federated.python.learning' has no attribute 'build_federated_averaging_process'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load and prepare the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_data = train_data.batch(32).shuffle(10000)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_data = test_data.batch(32)\n",
    "\n",
    "# Define the TensorFlow model\n",
    "def create_keras_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Set up the TFF simulation environment\n",
    "def model_fn():\n",
    "    keras_model = create_keras_model()\n",
    "    return tff.learning.from_keras_model(keras_model, input_spec=train_data.element_spec, loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "trainer = tff.learning.build_federated_averaging_process(model_fn)\n",
    "\n",
    "# Train the model using the TFF simulation\n",
    "state = trainer.initialize()\n",
    "for i in range(10):\n",
    "    state, metrics = trainer.next(state, [train_data] * 10)\n",
    "    print('Round {}: loss={}, accuracy={}'.format(i, metrics.loss, metrics.sparse_categorical_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff32671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from scipy import special\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Set verbosity.\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackResultsCollection\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import PrivacyMetric\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import PrivacyReportMetadata\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import privacy_report\n",
    "import tensorflow_privacy\n",
    "\n",
    "import argparse\n",
    "# Create an ArgumentParser object\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='number of classes')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "# set the value of num_classes manually\n",
    "args.num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20fcfdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 19:42:52.184416: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (50000, 32, 32, 3)\n",
      "y_train (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "activation = 'relu'\n",
    "num_conv = 3\n",
    "\n",
    "batch_size=50\n",
    "epochs_per_report = 2\n",
    "total_epochs = 50\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "train_ds = tfds.as_numpy(\n",
    "    tfds.load(dataset, split=tfds.Split.TRAIN, batch_size=-1))\n",
    "test_ds = tfds.as_numpy(\n",
    "    tfds.load(dataset, split=tfds.Split.TEST, batch_size=-1))\n",
    "x_train = train_ds['image'].astype('float32') / 255.\n",
    "y_train_indices = train_ds['label'][:, np.newaxis]\n",
    "x_test = test_ds['image'].astype('float32') / 255.\n",
    "y_test_indices = test_ds['label'][:, np.newaxis]\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train_indices, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_indices, num_classes)\n",
    "\n",
    "print('x_train', np.shape(x_train))\n",
    "print('y_train', np.shape(y_train))\n",
    "\n",
    "input_shape = x_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5f00f8",
   "metadata": {},
   "source": [
    "## test branchy net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e0827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from scipy import special\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Set verbosity.\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackResultsCollection\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import PrivacyMetric\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import PrivacyReportMetadata\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import privacy_report\n",
    "import tensorflow_privacy\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e16e4d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset.\n",
      "x_train (50000, 32, 32, 3)\n",
      "y_train (50000, 10)\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 0 Loss: 7.733941875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 1 Loss: 7.644310625 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 2 Loss: 7.6871825 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 3 Loss: 7.72679875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 4 Loss: 7.72679875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 5 Loss: 7.72679875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 6 Loss: 7.72679875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 7 Loss: 7.72679875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 8 Loss: 7.72679875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 9 Loss: 7.72679875 Accuracy: 0.9\n",
      "metric.result().numpy() 0.9\n",
      "Epoch: 10 Loss: 7.72679875 Accuracy: 0.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 202>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# Compute the total loss\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m losses:\n\u001b[0;32m--> 222\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m#### This should be a validation/test for the loss value need midify\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;66;03m# Collect the required logs in a dictionary\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric.result().numpy()\u001b[39m\u001b[38;5;124m\"\u001b[39m, metric\u001b[38;5;241m.\u001b[39mresult()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1407\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1403\u001b[0m   \u001b[38;5;66;03m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[0;32m-> 1407\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1409\u001b[0m   \u001b[38;5;66;03m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;66;03m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1413\u001b[0m   \u001b[38;5;66;03m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m   \u001b[38;5;66;03m# informative.\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__r\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m op_name):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1757\u001b[0m, in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1755\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m gen_math_ops\u001b[38;5;241m.\u001b[39madd(x, y, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:462\u001b[0m, in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    461\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m(\n\u001b[1;32m    463\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAddV2\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, x, y)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    465\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from scipy import special\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Set verbosity.\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import membership_inference_attack as mia\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackResultsCollection\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackType\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import PrivacyMetric\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import PrivacyReportMetadata\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import SlicingSpec\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import privacy_report\n",
    "import tensorflow_privacy\n",
    "\n",
    "import argparse\n",
    "\n",
    "class BranchyAlexNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BranchyAlexNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.conv1 = Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='valid')\n",
    "        self.pool1 = MaxPooling2D((3,3), strides=(2,2))\n",
    "        self.conv2 = Conv2D(256, (5,5), strides=(1,1), activation='relu', padding='same')\n",
    "        self.pool2 = MaxPooling2D((3,3), strides=(2,2))\n",
    "        self.conv3 = Conv2D(384, (3,3), strides=(1,1), activation='relu', padding='same')\n",
    "        self.conv4 = Conv2D(384, (3,3), strides=(1,1), activation='relu', padding='same')\n",
    "        self.conv5 = Conv2D(256, (3,3), strides=(1,1), activation='relu', padding='same')\n",
    "        self.pool3 = MaxPooling2D((3,3), strides=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(4096, activation='relu')\n",
    "        self.fc2 = Dense(4096, activation='relu')\n",
    "        self.fc3 = Dense(num_classes)\n",
    "        \n",
    "        self.branch1_fc = Dense(num_classes, name='branch1_fc')\n",
    "        self.branch2_fc = Dense(num_classes, name='branch2_fc')\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        final_output = self.fc3(x)\n",
    "        branch1_output = self.branch1_fc(x)\n",
    "        branch2_output = self.branch2_fc(x)\n",
    "        \n",
    "        if training:\n",
    "            return final_output, branch1_output, branch2_output\n",
    "        else:\n",
    "            return final_output\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='number of classes')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# set the value of num_classes manually\n",
    "args.num_classes = 10\n",
    "\n",
    "class branchy_CNNCifar(tf.keras.Model):\n",
    "    def __init__(self, args):\n",
    "        super(branchy_CNNCifar, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(6, (5, 5), activation='relu')\n",
    "        self.pool = tf.keras.layers.MaxPooling2D((2, 2))\n",
    "        self.conv2 = tf.keras.layers.Conv2D(16, (5, 5), activation='relu')\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(120, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(84, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(args.num_classes)\n",
    "\n",
    "        # Define the branches\n",
    "        self.branches = []\n",
    "\n",
    "        branch1 = tf.keras.Sequential([tf.keras.layers.Dense(64, activation='relu'),\n",
    "                                        tf.keras.layers.Dense(args.num_classes)])\n",
    "\n",
    "        self.branches.append(branch1)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, dtype=tf.float32)  # cast input to float32\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        branch = self.branches[0]\n",
    "        branch1_output = branch(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return branch1_output, x\n",
    "\n",
    "dataset = 'cifar10'\n",
    "num_classes = 10\n",
    "activation = 'relu'\n",
    "num_conv = 3\n",
    "\n",
    "batch_size=50\n",
    "epochs_per_report = 1\n",
    "total_epochs = 5\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "\n",
    "print('Loading the dataset.')\n",
    "train_ds = tfds.as_numpy(\n",
    "    tfds.load(dataset, split=tfds.Split.TRAIN, batch_size=-1))\n",
    "test_ds = tfds.as_numpy(\n",
    "    tfds.load(dataset, split=tfds.Split.TEST, batch_size=-1))\n",
    "x_train = train_ds['image'].astype('float32') / 255.\n",
    "y_train_indices = train_ds['label'][:, np.newaxis]\n",
    "x_test = test_ds['image'].astype('float32') / 255.\n",
    "y_test_indices = test_ds['label'][:, np.newaxis]\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = tf.keras.utils.to_categorical(y_train_indices, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test_indices, num_classes)\n",
    "\n",
    "print('x_train', np.shape(x_train))\n",
    "print('y_train', np.shape(y_train))\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "assert x_train.shape[0] % batch_size == 0, \"The tensorflow_privacy optimizer doesn't handle partial batches\"\n",
    "\n",
    "\n",
    "# Create the BranchyNet model\n",
    "model = branchy_CNNCifar(args)\n",
    "\n",
    "# Define your loss function\n",
    "def cross_entropy_loss(y_true, branch_output, final_output):\n",
    "\n",
    "    loss_early = tf.keras.losses.categorical_crossentropy(y_true, branch_output)\n",
    "    loss_final = tf.keras.losses.categorical_crossentropy(y_true, final_output)\n",
    "        # Compute the total loss\n",
    "    total_loss = loss_early*0.4 + loss_final*0.6\n",
    "    return total_loss\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define your accuracy metric\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# Define the early exit threshold\n",
    "threshold = 0.7\n",
    "# Define the training loop\n",
    "\n",
    "def train_step(inputs, labels):\n",
    "    # Initialize the gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        branch_output, final_output = model(inputs)\n",
    "\n",
    "        # Compute the total loss\n",
    "        total_loss =  cross_entropy_loss(labels, branch_output, final_output)\n",
    "        # print(total_loss)\n",
    "        # Compute the gradients\n",
    "        grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "        # Update the model\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "        # Convert the one-hot encoded labels to integer labels\n",
    "        labels = tf.argmax(labels, axis=1)\n",
    "        # Convert the integer labels to one-hot encoded labels\n",
    "        labels = tf.one_hot(labels, depth=10)\n",
    "        branch_output = tf.argmax(branch_output, axis =1)\n",
    "        final_output = tf.argmax(final_output, axis = 1)\n",
    "\n",
    "        metric.update_state(labels, final_output)\n",
    "        return final_output, total_loss\n",
    "        \n",
    "all_reports = []\n",
    "epochs_per_report = 1\n",
    "# callback = PrivacyMetrics(epochs_per_report, \"branchy_cnn\", model)\n",
    "# Train the model\n",
    "for epoch in range(total_epochs):\n",
    "    total_loss = 0\n",
    "    # Shuffle the training data\n",
    "    permutation = np.random.permutation(len(x_train))\n",
    "    x_train_shuffled = x_train[permutation]\n",
    "    y_train_shuffled = y_train[permutation]\n",
    "\n",
    "    for batch in range(0, len(x_train), batch_size):\n",
    "        # Get the batch\n",
    "        x_batch = x_train[batch:batch+batch_size]\n",
    "        y_batch = y_train[batch:batch+batch_size]\n",
    "\n",
    "\n",
    "        # Reset the accuracy metric\n",
    "        metric.reset_states()\n",
    "        # Call the training step function\n",
    "        outputs, losses = train_step(x_batch, y_batch)\n",
    "        \n",
    "        # Compute the total loss\n",
    "        for loss in losses:\n",
    "            total_loss += loss\n",
    "            \n",
    "        # Print the results\n",
    "\n",
    "    #### This should be a validation/test for the loss value need midify\n",
    "    # Collect the required logs in a dictionary\n",
    "    print(\"metric.result().numpy()\", metric.result().numpy())\n",
    "    logs = {'loss': total_loss.numpy()/len(x_train), 'val_accuracy': metric.result().numpy()}\n",
    "    # Call the on_epoch_end method with the logs dictionary\n",
    "    # callback.on_epoch_end(epoch, logs=logs)\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", total_loss.numpy()/len(x_train), \"Accuracy:\", metric.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026f0e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607756c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95011d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
